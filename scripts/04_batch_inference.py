#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""Batch Inference: Line-Level Visual Correction (MVP Style).

RESCUE PLAN:
1. Agent A detects LINES (not chars).
2. If Line Entropy is high -> Agent B reads the WHOLE LINE crop.
3. Use CER for evaluation.
"""

import os
import sys
from pathlib import Path

# Add project root to path FIRST
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Configure Logging EARLY (before other imports that might use logger)
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)

# --- HuggingFace æ¨¡å‹è·¯å¾„å’Œé•œåƒé…ç½® ---
# è‡ªåŠ¨æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ my_models æ–‡ä»¶å¤¹
MY_MODELS_DIR = PROJECT_ROOT / "my_models"

# å¦‚æœ my_models ç›®å½•å­˜åœ¨ï¼Œè®¾ç½® HF_HOME
if MY_MODELS_DIR.exists():
    os.environ["HF_HOME"] = str(MY_MODELS_DIR)
    logger.info(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç›®å½•: {MY_MODELS_DIR}")
else:
    # å¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤çš„ HuggingFace cache ç›®å½•
    # ä¸è®¾ç½® HF_HOMEï¼Œè®© HuggingFace ä½¿ç”¨é»˜è®¤ä½ç½®
    logger.info("âš ï¸  my_models ç›®å½•æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤ HuggingFace ç¼“å­˜ç›®å½•")

# é…ç½®é•œåƒç«™ç‚¹ï¼ˆå¦‚æœæœªè®¾ç½®ï¼‰
# æ”¯æŒçš„é•œåƒï¼šhf-mirror.comï¼ˆæ¨èï¼‰ã€openxlabã€modelscope
if "HF_ENDPOINT" not in os.environ:
    # é»˜è®¤ä½¿ç”¨ hf-mirror.comï¼ˆå›½å†…è®¿é—®å‹å¥½ï¼‰
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    logger.info("âœ… å·²è®¾ç½® HuggingFace é•œåƒ: https://hf-mirror.com")
    logger.info("ğŸ’¡ å¦‚éœ€ä½¿ç”¨å…¶ä»–é•œåƒï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡: export HF_ENDPOINT=<é•œåƒURL>")
else:
    logger.info(f"âœ… ä½¿ç”¨å·²é…ç½®çš„é•œåƒ: {os.environ['HF_ENDPOINT']}")

# ç¦»çº¿æ¨¡å¼ï¼šé»˜è®¤ç¦ç”¨ï¼Œå…è®¸åœ¨çº¿ä¸‹è½½æ¨¡å‹
# åªæœ‰åœ¨æ˜ç¡®è®¾ç½®ç¯å¢ƒå˜é‡æ—¶æ‰å¯ç”¨ç¦»çº¿æ¨¡å¼
HF_OFFLINE = os.environ.get("HF_HUB_OFFLINE", "")
if HF_OFFLINE == "" or HF_OFFLINE == "0":
    # ç¡®ä¿æœªè®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œå…è®¸åœ¨çº¿ä¸‹è½½
    os.environ.pop("HF_HUB_OFFLINE", None)
    logger.info("ğŸŒ åœ¨çº¿æ¨¡å¼å·²å¯ç”¨ï¼ˆå°†ä»é•œåƒç«™ç‚¹ä¸‹è½½æ¨¡å‹ï¼‰")
else:
    logger.info(f"ğŸ“¦ ç¦»çº¿æ¨¡å¼å·²å¯ç”¨: HF_HUB_OFFLINE={HF_OFFLINE}")

# Now import other modules
import argparse
import json
from typing import Dict

import cv2
import numpy as np
import Levenshtein
from tqdm import tqdm

from core.agent_a import AgentA
from core.agent_b import AgentB


def normalize_text(text: str) -> str:
    """Normalize text for CER calculation."""
    if not text:
        return ""
    # Remove whitespace and common punctuation noise
    return text.strip().replace(" ", "").replace("\n", "")


class L2W1Pipeline:
    def __init__(self, agent_b_model_path: str = None):
        """Initialize L2W1 Pipeline.

        Args:
            agent_b_model_path: Agent B æ¨¡å‹è·¯å¾„ã€‚
                - å¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤ "openbmb/MiniCPM-V-4_5" (SOTA model)
                - å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ç¦»çº¿æ¨¡å¼
                - å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ HF_HOME æŒ‡å®šæ¨¡å‹ç›®å½•
        """
        logger.info("Initializing L2W1 Pipeline (Line-Level Mode)...")
        # 1. Agent A (The Scout) - Force Detection ON
        self.agent_a = AgentA(use_gpu=True)

        # 2. Agent B (The Judge)
        # ä¼˜å…ˆä½¿ç”¨å‚æ•°æŒ‡å®šçš„è·¯å¾„ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        if agent_b_model_path is None:
            # å°è¯•ä»ç¯å¢ƒå˜é‡æˆ–æœ¬åœ°ç›®å½•æŸ¥æ‰¾æ¨¡å‹
            if MY_MODELS_DIR.exists():
                # æŸ¥æ‰¾ my_models ç›®å½•ä¸‹çš„ MiniCPM-V æ¨¡å‹ï¼ˆä¼˜å…ˆï¼‰æˆ– Qwen2-VL æ¨¡å‹ï¼ˆå…¼å®¹ï¼‰
                potential_paths = list(MY_MODELS_DIR.glob("*MiniCPM*V*"))
                if not potential_paths:
                    # Fallback: æŸ¥æ‰¾ Qwen2-VL æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰
                    potential_paths = list(MY_MODELS_DIR.glob("*Qwen*VL*"))
                if potential_paths:
                    agent_b_model_path = str(potential_paths[0])
                    logger.info(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹: {agent_b_model_path}")
                else:
                    # [FIX] ä½¿ç”¨ SOTA æ¨¡å‹ MiniCPM-V-4_5 ä½œä¸ºé»˜è®¤å€¼
                    agent_b_model_path = "openbmb/MiniCPM-V-4_5"
            else:
                # [FIX] ä½¿ç”¨ SOTA æ¨¡å‹ MiniCPM-V-4_5 ä½œä¸ºé»˜è®¤å€¼
                agent_b_model_path = "openbmb/MiniCPM-V-4_5"

        self.agent_b = AgentB(model_path=agent_b_model_path, load_in_4bit=True)

    def run(self, image_path: str, gt_text: str = "") -> Dict:
        """Process a single image."""
        # --- Step 1: Agent A (Detection + Recognition) ---
        # We use standard inference, which returns LINES
        # skip_detection=False is CRITICAL here
        results = self.agent_a.inference(image_path, skip_detection=False)

        # Merge all lines into one prediction (for full text evaluation)
        full_pred_a = ""
        full_pred_final = ""

        line_logs = []
        is_routed_any = False

        # Load original image for cropping
        img = cv2.imread(image_path)
        if img is None:
            return {"error": "Image load failed"}

        for line in results:
            text_a = line["text"]
            box = line["box"]
            avg_entropy = line["avg_entropy"]

            # --- Step 2: Router Strategy (Line Level) ---
            # If the line is uncertain (Entropy > 0.001) or contains specific keywords
            # For this rescue run, we use a low threshold to force Agent B to work
            is_routed = avg_entropy > 0.001 or len(text_a) < 2

            final_text = text_a

            if is_routed:
                is_routed_any = True
                # --- Step 3: Agent B (Visual Correction) ---
                # Crop the WHOLE line
                xs = [p[0] for p in box]
                ys = [p[1] for p in box]
                x_min, x_max = int(min(xs)), int(max(xs))
                y_min, y_max = int(min(ys)), int(max(ys))

                # Padding to capture context
                h_img, w_img = img.shape[:2]
                pad = 10
                y_min = max(0, y_min - pad)
                y_max = min(h_img, y_max + pad)
                x_min = max(0, x_min - pad)
                x_max = min(w_img, x_max + pad)

                line_crop = img[y_min:y_max, x_min:x_max]

                # Convert to PIL for Agent B
                from PIL import Image

                line_crop_pil = Image.fromarray(
                    cv2.cvtColor(line_crop, cv2.COLOR_BGR2RGB)
                )

                # Agent B Prompt: Ask to read the line
                # We use a custom call to Agent B's internal model or modify the prompt slightly
                # Here we reuse the interface but treat context as empty
                try:
                    # Call Agent B to correct the OCR prediction
                    # Agent B will use its V-CoT prompt internally
                    corrected = self.agent_b.inference(
                        crop_image=line_crop_pil,
                        context_left="",  # No external context needed for line
                        context_right="",
                        ocr_pred=text_a,
                    )

                    if (
                        corrected
                        and corrected != text_a
                        and "<UNKNOWN>" not in corrected
                    ):
                        final_text = corrected

                except Exception as e:
                    logger.error(f"Agent B failed: {e}")

            full_pred_a += text_a
            full_pred_final += final_text

            line_logs.append({"ocr": text_a, "l2w1": final_text, "routed": is_routed})

        # --- Step 4: Evaluation ---
        norm_gt = normalize_text(gt_text)
        norm_a = normalize_text(full_pred_a)
        norm_final = normalize_text(full_pred_final)

        cer_a = Levenshtein.distance(norm_a, norm_gt) / len(norm_gt) if norm_gt else 1.0
        cer_final = (
            Levenshtein.distance(norm_final, norm_gt) / len(norm_gt) if norm_gt else 1.0
        )

        return {
            "id": Path(image_path).name,
            "gt": gt_text,
            "pred_a": full_pred_a,
            "pred_final": full_pred_final,
            "cer_a": cer_a,
            "cer_final": cer_final,
            "is_routed": is_routed_any,
        }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--test_set", type=str, required=True)
    parser.add_argument("--output_csv", type=str, default="output/rescue_result.csv")
    parser.add_argument(
        "--agent_b_model",
        type=str,
        default=None,
        help="Agent B æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– HuggingFace IDï¼‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†è‡ªåŠ¨æ£€æµ‹ my_models ç›®å½•",
    )
    args = parser.parse_args()

    pipeline = L2W1Pipeline(agent_b_model_path=args.agent_b_model)

    with open(args.test_set, "r") as f:
        data = json.load(f)
        samples = data.get("samples", [])

    results = []
    print(f"\nğŸš€ Running Rescue Protocol on {len(samples)} samples...\n")

    for sample in tqdm(samples):
        res = pipeline.run(sample["image_path"], sample["label_gt"])
        results.append(res)

        # Print Bad Cases (where Baseline failed)
        if res["cer_a"] > 0.1:
            print(f"\n[Bad Case] {res['id']}")
            print(f"  GT  : {res['gt']}")
            print(f"  OCR : {res['pred_a']}")
            print(f"  L2W1: {res['pred_final']}")

    # Calculate Stats
    avg_cer_a = np.mean([r["cer_a"] for r in results])
    avg_cer_final = np.mean([r["cer_final"] for r in results])

    print("\n" + "=" * 50)
    print(" ğŸ“Š RESCUE RESULTS (Line-Level Logic)")
    print("=" * 50)
    print(f" Total Samples   : {len(samples)}")
    print(f" Baseline CER    : {avg_cer_a:.4f}")
    print(f" L2W1 CER        : {avg_cer_final:.4f}")
    print(f" Improvement     : {(avg_cer_a - avg_cer_final) * 100:.2f}%")
    print("=" * 50)

    # Save CSV
    import pandas as pd

    df = pd.DataFrame(results)
    df.to_csv(args.output_csv, index=False)
    print(f"Saved results to {args.output_csv}")


if __name__ == "__main__":
    main()
