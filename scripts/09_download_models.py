#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""ä¸‹è½½ HuggingFace æ¨¡å‹åˆ°æœ¬åœ°ï¼Œç”¨äºç¦»çº¿éƒ¨ç½²ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    # ä¸‹è½½ Agent B æ¨¡å‹
    python scripts/09_download_models.py --model Qwen/Qwen2-VL-2B-Instruct --output my_models

    # ä¸‹è½½ Router æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    python scripts/09_download_models.py --model Qwen/Qwen2.5-0.5B-Instruct --output my_models

    # ä½¿ç”¨é•œåƒç«™ç‚¹ä¸‹è½½ï¼ˆæ¨èï¼‰
    python scripts/09_download_models.py --model Qwen/Qwen2-VL-2B-Instruct --output my_models --mirror hf-mirror.com
"""

import argparse
import os
import sys
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)

# æ”¯æŒçš„é•œåƒç«™ç‚¹
MIRROR_SITES = {
    "hf-mirror.com": "https://hf-mirror.com",
    "hf-mirror": "https://hf-mirror.com",
    "openxlab": "https://code.openxlab.org.cn",
    "modelscope": "https://www.modelscope.cn",
}


def setup_mirror(mirror_name: str = None) -> None:
    """é…ç½® HuggingFace é•œåƒç«™ç‚¹ã€‚
    
    Args:
        mirror_name: é•œåƒç«™ç‚¹åç§°ï¼Œæ”¯æŒ: hf-mirror.com, openxlab, modelscope
    """
    if mirror_name and mirror_name in MIRROR_SITES:
        mirror_url = MIRROR_SITES[mirror_name]
        os.environ["HF_ENDPOINT"] = mirror_url
        logger.info(f"âœ… å·²è®¾ç½® HuggingFace é•œåƒ: {mirror_url}")
    elif mirror_name:
        # è‡ªå®šä¹‰é•œåƒ URL
        os.environ["HF_ENDPOINT"] = mirror_name
        logger.info(f"âœ… å·²è®¾ç½®è‡ªå®šä¹‰é•œåƒ: {mirror_name}")
    else:
        # é»˜è®¤ä½¿ç”¨ hf-mirror.com
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
        logger.info("âœ… å·²è®¾ç½®é»˜è®¤é•œåƒ: https://hf-mirror.com")


def download_model(model_name: str, output_dir: Path, use_mirror: bool = True) -> None:
    """ä¸‹è½½ HuggingFace æ¨¡å‹åˆ°æŒ‡å®šç›®å½•ã€‚
    
    Args:
        model_name: HuggingFace æ¨¡å‹æ ‡è¯†ç¬¦ï¼Œå¦‚ "Qwen/Qwen2-VL-2B-Instruct"
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        use_mirror: æ˜¯å¦ä½¿ç”¨é•œåƒç«™ç‚¹
    """
    try:
        from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer
        from transformers import Qwen2VLForConditionalGeneration
    except ImportError:
        logger.error("âŒ è¯·å…ˆå®‰è£… transformers: pip install transformers")
        sys.exit(1)
    
    # è®¾ç½®é•œåƒï¼ˆå¦‚æœéœ€è¦ï¼‰
    if use_mirror:
        setup_mirror()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¨¡å‹ä¿å­˜è·¯å¾„
    model_dir = output_dir / model_name.replace("/", "_")
    
    logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
    logger.info(f"ğŸ“ ä¿å­˜è·¯å¾„: {model_dir}")
    
    try:
        # åˆ¤æ–­æ¨¡å‹ç±»å‹
        is_vlm = "VL" in model_name or "vision" in model_name.lower()
        
        if is_vlm:
            # VLM æ¨¡å‹éœ€è¦ä¸‹è½½ processor å’Œ model
            logger.info("æ£€æµ‹åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸‹è½½ Processor å’Œ Model...")
            
            processor = AutoProcessor.from_pretrained(
                model_name,
                trust_remote_code=True,
                cache_dir=str(output_dir),
            )
            processor.save_pretrained(str(model_dir))
            logger.info("âœ… Processor ä¸‹è½½å®Œæˆ")
            
            model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_name,
                trust_remote_code=True,
                cache_dir=str(output_dir),
                torch_dtype="auto",
            )
            model.save_pretrained(str(model_dir))
            logger.info("âœ… Model ä¸‹è½½å®Œæˆ")
            
        else:
            # æ™®é€šè¯­è¨€æ¨¡å‹
            logger.info("æ£€æµ‹åˆ°è¯­è¨€æ¨¡å‹ï¼Œä¸‹è½½ Tokenizer å’Œ Model...")
            
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                cache_dir=str(output_dir),
            )
            tokenizer.save_pretrained(str(model_dir))
            logger.info("âœ… Tokenizer ä¸‹è½½å®Œæˆ")
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                cache_dir=str(output_dir),
                torch_dtype="auto",
            )
            model.save_pretrained(str(model_dir))
            logger.info("âœ… Model ä¸‹è½½å®Œæˆ")
        
        logger.info(f"\nğŸ‰ æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
        logger.info(f"ğŸ“¦ æ¨¡å‹è·¯å¾„: {model_dir}")
        logger.info(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        logger.info(f"   1. å°†æ•´ä¸ª '{output_dir.name}' æ–‡ä»¶å¤¹ä¸Šä¼ åˆ°æœåŠ¡å™¨")
        logger.info(f"   2. åœ¨æœåŠ¡å™¨ä¸Šè®¾ç½®ç¯å¢ƒå˜é‡: export HF_HOME=/path/to/{output_dir.name}")
        logger.info(f"   3. æˆ–è€…ä¿®æ”¹ä»£ç ä¸­çš„ model_path ä¸ºæœ¬åœ°è·¯å¾„")
        
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        logger.error("\nğŸ’¡ æç¤º:")
        logger.error("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        logger.error("   2. å°è¯•ä½¿ç”¨é•œåƒç«™ç‚¹: --mirror hf-mirror.com")
        logger.error("   3. å¦‚æœé•œåƒä¹Ÿå¤±è´¥ï¼Œå¯ä»¥æ‰‹åŠ¨ä» https://huggingface.co ä¸‹è½½")
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description="ä¸‹è½½ HuggingFace æ¨¡å‹åˆ°æœ¬åœ°ï¼Œç”¨äºç¦»çº¿éƒ¨ç½²"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="Qwen/Qwen2-VL-2B-Instruct",
        help="HuggingFace æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆé»˜è®¤: Qwen/Qwen2-VL-2B-Instructï¼‰",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="my_models",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: my_modelsï¼‰",
    )
    parser.add_argument(
        "--mirror",
        type=str,
        default="hf-mirror.com",
        help="é•œåƒç«™ç‚¹ï¼ˆé»˜è®¤: hf-mirror.comï¼Œå¯é€‰: openxlab, modelscopeï¼‰",
    )
    parser.add_argument(
        "--no-mirror",
        action="store_true",
        help="ä¸ä½¿ç”¨é•œåƒï¼Œç›´æ¥è¿æ¥ HuggingFaceï¼ˆéœ€è¦ç§‘å­¦ä¸Šç½‘ï¼‰",
    )
    
    args = parser.parse_args()
    
    output_dir = Path(args.output).resolve()
    
    download_model(
        model_name=args.model,
        output_dir=output_dir,
        use_mirror=not args.no_mirror,
    )


if __name__ == "__main__":
    main()

